{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi dataset - anomaly detection with Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'ansible (Python 3.9.20)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n ansible ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, Dropout, TimeDistributed\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "```nyc_taxi.csv``` - number of NYC taxi passengers, where the five anomalies occur during the NYC marathon, Thanksgiving, Christmas, New Years Day, and a snow storm. The raw data is from the [NYC Taxi and Limousine Commission](https://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml). The data consists of aggregating the total number of taxi passengers into 30 minute buckets.\n",
    "\n",
    "![info](https://raw.githubusercontent.com/bartk97/NYC-Taxi-Anomaly-Detection/main/Images/Data%20with%20highlighted%20anomalies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "I am going to detect anomalies in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "Importing data from https://github.com/numenta/NAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv'\n",
    "data = pd.read_csv(url, parse_dates=['timestamp'], index_col='timestamp')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is split into 30 minute buckets so there are $24\\cdot2 = 48$ data points per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "period = 24 * 2\n",
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('From  ' + str(np.min(data.index)) + '  to  ' +str(np.max(data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size: %d \\nNumber of data per day: %d \\nNumber of days: %d' %(data.shape[0], period, data.shape[0] / period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing value: ', data.isnull().to_numpy().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(title='Data points', figsize=(30,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots with the data from first 5 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    data[period*i:period*(i + 1)].plot(figsize=(5, 2), title=str(data.index[period * i])[:10])\n",
    "    plt.xlabel(None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into days\n",
    "\n",
    "I split a time series into days and created a new DataFrame as follows: one row corresponds to one day and one column corresponds to 30-minute interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into days - function\n",
    "def create_dataset(X, dates, period=1):\n",
    "    Xs = []\n",
    "    indexes = []\n",
    "    for i in range(int(len(X) / period)):\n",
    "        v = X.iloc[i*period: (i + 1)*period].values\n",
    "        indexes.append(dates[period*i])\n",
    "        Xs.append(v)        \n",
    "    return np.array(Xs), np.array(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame\n",
    "df, dates = create_dataset(data.value, data.index, period)\n",
    "df = pd.DataFrame(df, dates)\n",
    "\n",
    "print('df.shape: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and standard deviation function (w.r.t. hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.mean(), label='mean')\n",
    "plt.fill_between(df.columns, df.mean() + df.std(), df.mean() - df.std(), alpha=0.3, label='$\\pm$ std')\n",
    "plt.fill_between(df.columns, df.mean() + 2*df.std(), df.mean() - 2*df.std(), alpha=0.1, label= '$\\pm$ 2std')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and standard deviation function vs Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 15\n",
    "mean_fucntion = np.tile(df.mean(), n_days)\n",
    "std_function = np.tile(df.std(), n_days)\n",
    "\n",
    "plt.figure(figsize=(30, 4))\n",
    "plt.plot(mean_fucntion, label='mean')\n",
    "plt.fill_between(np.arange(period*n_days), mean_fucntion - std_function, mean_fucntion + std_function, alpha=0.2, label='std')\n",
    "plt.plot(data.to_numpy().flatten()[:period*n_days], label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "ratio = 0.55\n",
    "train_size = int(df.shape[0] * ratio)\n",
    "X_train = df[:train_size]\n",
    "X_test = df[train_size:]\n",
    "\n",
    "dates_train = np.array(df.index[:train_size], dtype='datetime64[D]')\n",
    "dates_test = np.array(df.index[train_size:], dtype='datetime64[D]')\n",
    "\n",
    "\n",
    "# info\n",
    "print('Train size: ', ratio)\n",
    "print('\\n\\nTRAIN SET:  from  ' + str(np.min(dates_train)) + '  to  ' +str(np.max(dates_train)))\n",
    "print('Data size: ', X_train.shape[0])\n",
    "print('Number of days: ', int(X_train.shape[0] / period))\n",
    "print('\\n\\nTEST SET:  from  ' + str(np.min(dates_test)) + '  to  ' +str(np.max(dates_test)))\n",
    "print('Data size: ', X_test.shape[0])\n",
    "print('Number of days: ', int(X_test.shape[0] / period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots\n",
    "plt.figure(figsize=(30,4))\n",
    "plt.title('Train set')\n",
    "plt.plot(X_train.to_numpy().flatten())\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,4))\n",
    "plt.title('Test set')\n",
    "plt.plot(X_test.to_numpy().flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the test set we have events such as the NYC marathon, Thanksgiving, Christmas, New Years Day, and a snow storm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data standardization\n",
    "Standardize data by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Before standarize')\n",
    "plt.plot(scaler.inverse_transform(X_train[0]))\n",
    "plt.show()\n",
    "\n",
    "plt.title('After standarize')\n",
    "plt.plot(X_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "dim_hidden1 = 32\n",
    "dim_hidden2 = 16\n",
    "dim_hidden3 = 8\n",
    "\n",
    "\n",
    "# model\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Sequential([\n",
    "          Dense(dim_hidden1, activation=\"relu\"),\n",
    "          Dense(dim_hidden2, activation=\"relu\"),\n",
    "          Dense(dim_hidden3, activation=\"relu\")])\n",
    "\n",
    "        self.decoder = Sequential([\n",
    "          Dense(dim_hidden2, activation=\"relu\"),\n",
    "          Dense(dim_hidden1, activation=\"relu\"),\n",
    "          Dense(period, activation=\"sigmoid\")])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "epochs = 100\n",
    "batch_size = 20\n",
    "validation_split = 0.1\n",
    "shuffle = False\n",
    "\n",
    "\n",
    "# fitting model\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train, \n",
    "                          epochs = epochs, \n",
    "                          batch_size = batch_size, \n",
    "                          validation_split = validation_split, \n",
    "                          shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction example on a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = autoencoder.encoder(X_train).numpy()\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "\n",
    "i = 10\n",
    "\n",
    "plt.plot(X_train[i], label='One day from train set')\n",
    "plt.plot(decoded_data[i], label='Reconstruction')\n",
    "plt.fill_between(np.arange(48), decoded_data[i], X_train[i], alpha=0.2, label='Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Error (MSE): ',np.mean((X_train[i] - decoded_data[i])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction example on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = autoencoder.encoder(X_test).numpy()\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "\n",
    "i = 10\n",
    "\n",
    "plt.plot(X_test[i], label='One day from test set')\n",
    "plt.plot(decoded_data[i], label='Reconstruction')\n",
    "plt.fill_between(np.arange(48), decoded_data[i], X_test[i], alpha=0.2, label='Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Error (MSE): ',np.mean((X_test[i] - decoded_data[i])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss distribution - Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = autoencoder.predict(X_train)\n",
    "loss_train = tf.keras.losses.mae(reconstruction, X_train)\n",
    "\n",
    "plt.title('Loss distribution - train set')\n",
    "sns.distplot(loss_train, bins=50, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss distribution - Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_test = autoencoder.predict(X_test)\n",
    "loss_test = tf.keras.losses.mae(reconstruction_test, X_test)\n",
    "\n",
    "plt.title('Loss distribution - test set')\n",
    "sns.distplot(loss_test, bins=50, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Threshold\n",
    "\n",
    "threshold = mean of the loss + 2 standard devation of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# threshold = np.quantile(loss_train, 0.95)\n",
    "threshold = np.mean(loss_train) + 2*np.std(loss_train)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for each day of the test set vs Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'date': np.array(dates_test, dtype='datetime64[D]'), \n",
    "                        'loss': loss_test})\n",
    "\n",
    "results = results.set_index('date')\n",
    "\n",
    "results.plot(kind='bar', figsize=(30,4))\n",
    "plt.axhline(threshold, color='red', label='threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detected anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_loss = (loss_test - np.min(loss_test)) / (np.max(loss_test) - np.min(loss_test)) * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loss_test.numpy() >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,4))\n",
    "for i in np.arange(y_pred.shape[0])[y_pred]:\n",
    "    plt.axvspan(i*48, (i + 1)*48, color='r',alpha=scaled_loss[i])\n",
    "plt.plot(scaler.inverse_transform(X_test).flatten())\n",
    "plt.legend(['data', 'anomaly'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.array(dates_test[y_pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the detected anomalies  match the following events:\n",
    "* [NYC Marathon](https://en.wikipedia.org/wiki/2014_New_York_City_Marathon) (02.11.2014)\n",
    "* Thanksgiving (27.11.2014)\n",
    "* Christmas (24-26.12.2014)\n",
    "* New Years Day (01.01.2015)\n",
    "* [January 2015 North American blizzard](https://en.wikipedia.org/wiki/January_2015_North_American_blizzard) (26-27.01.2015)\n",
    "\n",
    "![image info](https://raw.githubusercontent.com/bartk97/NYC-Taxi-Anomaly-Detection/main/Images/Data%20with%20highlighted%20anomalies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of detected anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d, x in zip(np.array(dates_test[y_pred]), scaler.inverse_transform(X_test)[y_pred]):\n",
    "    plt.figure(figsize=(5,2))\n",
    "    plt.title(d)\n",
    "    plt.plot(x)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ansible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
